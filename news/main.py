import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import re
import pandas as pd
import sys

# Add the parent directory to sys.path to import TimescaleDBUtil
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.timescaledb_util import TimescaleDBUtil

# Create data directory if it doesn't exist
os.makedirs("data", exist_ok=True)

# ==== H√†m ki·ªÉm tra b√†i vi·∫øt c√≥ li√™n quan H√† N·ªôi kh√¥ng ====
def is_related_to_hanoi(title, content):
    keywords = ["H√† N·ªôi", "ha noi", "TP H√† N·ªôi", "tp. h√† n·ªôi", "th·ªß ƒë√¥", "n·ªôi th√†nh", "ngo·∫°i th√†nh"]
    text = (title + " " + content).lower()
    return any(keyword.lower() in text for keyword in keywords)

# ==== H√†m ki·ªÉm tra t·ª´ kh√≥a li√™n quan ƒë·∫øn kh√¥ng kh√≠ ====
def is_air_related(title, content):
    keywords = [ "b·ª•i", "b·ª•i m·ªãn", "PM2.5", "kh√≠ th·∫£i", "kh√≠ ƒë·ªôc", "ch√°y", "ch√°y r·ª´ng", 
    "ƒë·ªët", "ƒë·ªët r√°c", "ƒë·ªët r∆°m r·∫°", "√¥ nhi·ªÖm kh√¥ng kh√≠", "kh√≥i", "l√≤ ƒë·ªët", "nh√† m√°y", 
    "kh√¥ng kh√≠", "haze", "smog", "nhi·ªát ƒë·ªô kh√¥ng kh√≠", "s∆∞∆°ng m√π", "m√π kh√¥", "ngh·∫πt th·ªü", "CO2", "NO2"]
    text = (title + " " + content).lower()
    matched_keywords = [keyword for keyword in keywords if keyword.lower() in text]
    return matched_keywords

# ==== ƒê·ªçc c√°c link ƒë√£ crawl tr∆∞·ªõc ƒë√≥ t·ª´ file JSON ====
def read_existing_links(filename):
    links = set()
    try:
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
            for item in data:
                if "link" in item:
                    links.add(item["link"])
    except (FileNotFoundError, json.JSONDecodeError):
        pass
    return links

# ==== Crawl VnExpress (20 trang) ====
def crawl_vnexpress(max_pages=20, existing_links=None):
    if existing_links is None:
        existing_links = set()
    base_url = "https://vnexpress.net/chu-de/o-nhiem-moi-truong-6877"
    headers = {"User-Agent": "Mozilla/5.0"}
    data = []

    for i in range(1, max_pages + 1):
        url = base_url if i == 1 else f"{base_url}-p{i}"
        print(f"üåê VnExpress: {url}")
        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.content, "html.parser")
            articles = soup.select(".featured-item, .item-news")

            for article in articles:
                a_tag = article.select_one("h3.title-news a")
                if not a_tag:
                    continue
                title = a_tag.get_text(strip=True)
                link = a_tag["href"]

                if link in existing_links:
                    continue

                try:
                    detail_res = requests.get(link, headers=headers, timeout=10)
                    detail_soup = BeautifulSoup(detail_res.content, "html.parser")
                    content_tag = detail_soup.select_one(".fck_detail")
                    time_tag = detail_soup.select_one(".date")

                    if content_tag and time_tag:
                        content = content_tag.get_text(strip=True)
                        time = time_tag.get_text(strip=True)

                        if is_related_to_hanoi(title, content):
                            data.append({
                                "title": title,
                                "time": time,
                                "category": "VnExpress",
                                "content": content,
                                "link": link
                            })
                            existing_links.add(link)
                except Exception as e:
                    print(f"‚ùå Chi ti·∫øt VnExpress l·ªói: {link} - {e}")
        except Exception as e:
            print(f"‚ùå Trang VnExpress l·ªói: {url} - {e}")
            continue
    return data

# Crawl VnExpress H√† N·ªôi (20 trang)
def crawl_vnexpress_ha_noi(max_pages=20, existing_links=None):
    if existing_links is None:
        existing_links = set()
    base_url = "https://vnexpress.net/topic/ha-noi-26482"
    headers = {"User-Agent": "Mozilla/5.0"}
    data = []

    for i in range(1, max_pages + 1):
        url = base_url if i == 1 else f"{base_url}-p{i}"
        print(f"üåê VnExpress H√† N·ªôi: {url}")
        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.content, "html.parser")
            articles = soup.select(".featured-item, .item-news")

            for article in articles:
                a_tag = article.select_one("h3.title-news a")
                if not a_tag:
                    continue
                title = a_tag.get_text(strip=True)
                link = a_tag["href"]

                if link in existing_links:
                    continue

                try:
                    detail_res = requests.get(link, headers=headers, timeout=10)
                    detail_soup = BeautifulSoup(detail_res.content, "html.parser")
                    content_tag = detail_soup.select_one(".fck_detail")
                    time_tag = detail_soup.select_one(".date")

                    if content_tag and time_tag:
                        content = content_tag.get_text(strip=True)
                        time = time_tag.get_text(strip=True)

                        if is_related_to_hanoi(title, content):
                            data.append({
                                "title": title,
                                "time": time,
                                "category": "VnExpress",
                                "content": content,
                                "link": link
                            })
                            existing_links.add(link)
                except Exception as e:
                    print(f"‚ùå Chi ti·∫øt VnExpress l·ªói: {link} - {e}")
        except Exception as e:
            print(f"‚ùå Trang VnExpress l·ªói: {url} - {e}")
            continue
    return data

# ==== Crawl D√¢n Tr√≠ (30 trang) ====
def crawl_dantri(max_pages=30, existing_links=None):
    if existing_links is None:
        existing_links = set()
    headers = {"User-Agent": "Mozilla/5.0"}
    data = []
    seen_links = set()

    for page in range(1, max_pages + 1):
        if page == 1:
            url = "https://dantri.com.vn/xa-hoi/moi-truong.htm"
        else:
            url = f"https://dantri.com.vn/xa-hoi/moi-truong/trang-{page}.htm"

        print(f"üåê D√¢n Tr√≠: {url}")
        try:
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.content, "html.parser")
            articles = soup.select("article")

            for article in articles:
                a_tag = article.select_one("h3 a")
                if not a_tag:
                    continue
                link = a_tag["href"]
                title = a_tag.get_text(strip=True)

                # üõë Lo·∫°i tr√πng URL
                if link in seen_links or link in existing_links:
                    continue
                seen_links.add(link)
                existing_links.add(link)

                try:
                    detail_res = requests.get(link, headers=headers)
                    detail_soup = BeautifulSoup(detail_res.content, "html.parser")
                    content_tag = detail_soup.select_one(".singular-content")
                    time_tag = detail_soup.select_one(".author-time")

                    if content_tag and time_tag:
                        content = content_tag.get_text(strip=True)
                        time = time_tag.get_text(strip=True)

                        if is_related_to_hanoi(title, content):
                            data.append({
                                "title": title,
                                "time": time,
                                "category": "D√¢n Tr√≠",
                                "content": content,
                                "link": link
                            })
                except Exception as e:
                    print(f"‚ùå Chi ti·∫øt D√¢n Tr√≠ l·ªói: {link} - {e}")
        except Exception as e:
            print(f"‚ùå Trang D√¢n Tr√≠ l·ªói: {url} - {e}")
    return data

# ==== H√†m chu·∫©n h√≥a th·ªùi gian ====
def standardize_time(time_str):
    """Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng th·ªùi gian t·ª´ c√°c b√°o kh√°c nhau v·ªÅ d·∫°ng YYYY-MM-DD"""
    if not time_str:
        return datetime.now().strftime("%Y-%m-%d")
        
    time_str = time_str.lower().strip()
    
    # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng "Th·ª© ..., DD/MM/YYYY, HH:MM (GMT+7)"
    match = re.search(r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})', time_str)
    if match:
        day, month, year = match.groups()
        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    
    # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng "HH:MM - Ng√†y DD/MM/YYYY"
    match = re.search(r'ng√†y\s+(\d{1,2})[/-](\d{1,2})[/-](\d{4})', time_str)
    if match:
        day, month, year = match.groups()
        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    
    # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng ch·ªâ c√≥ ng√†y v√† th√°ng c·ªßa nƒÉm hi·ªán t·∫°i (DD/MM)
    match = re.search(r'(\d{1,2})[/-](\d{1,2})', time_str)
    if match:
        # Trong ti·∫øng Vi·ªát, ƒë·ªãnh d·∫°ng th∆∞·ªùng l√† ng√†y/th√°ng
        day, month = int(match.group(1)), int(match.group(2))
        
        # Ki·ªÉm tra v√† ƒë·∫£m b·∫£o day <= 31 v√† month <= 12
        if day > 31 or month > 12:
            # N·∫øu c√≥ l·ªói, ƒë·∫£o ng∆∞·ª£c day v√† month
            day, month = month, day
            
        # N·∫øu v·∫´n kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng gi√° tr·ªã an to√†n
        day = min(day, 31)
        month = min(month, 12)
        
        current_year = datetime.now().year
        return f"{current_year}-{str(month).zfill(2)}-{str(day).zfill(2)}"
    
    # N·∫øu kh√¥ng kh·ªõp v·ªõi b·∫•t k·ª≥ m·∫´u n√†o, tr·∫£ v·ªÅ th·ªùi gian hi·ªán t·∫°i
    return datetime.now().strftime("%Y-%m-%d")

# ==== L∆∞u d·ªØ li·ªáu v√†o file JSON ====
def save_to_json(data, filename):
    # ƒê·ªçc d·ªØ li·ªáu c≈© n·∫øu file t·ªìn t·∫°i
    existing_data = []
    existing_links = set()
    try:
        with open(filename, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
            for item in existing_data:
                if "link" in item:
                    existing_links.add(item["link"])
    except (FileNotFoundError, json.JSONDecodeError):
        pass
    
    # K·∫øt h·ª£p d·ªØ li·ªáu c≈© v√† m·ªõi, lo·∫°i tr√πng theo link
    new_data = []
    for item in data:
        if "link" in item and item["link"] not in existing_links:
            new_data.append(item)
            existing_links.add(item["link"])
    combined_data = existing_data + new_data
    
    # L∆∞u d·ªØ li·ªáu v√†o file
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(combined_data, f, ensure_ascii=False, indent=2)

# ==== H√†m l·ªçc b√†i vi·∫øt li√™n quan ƒë·∫øn kh√¥ng kh√≠ ====
def filter_air_articles(input_file, output_file):
    filtered = []

    try:
        with open(input_file, "r", encoding="utf-8") as infile:
            data = json.load(infile)
            
            for item in data:
                title = item.get("title", "")
                content = item.get("content", "")
                time_str = item.get("time", "")
                
                # Chu·∫©n h√≥a th·ªùi gian
                item["standardized_time"] = standardize_time(time_str)
                
                matched_keywords = is_air_related(title, content)

                if matched_keywords:
                    item["keyword"] = ", ".join(matched_keywords)
                    filtered.append(item)

        if filtered:
            with open(output_file, "w", encoding="utf-8") as outfile:
                json.dump(filtered, outfile, ensure_ascii=False, indent=2)
            print(f"‚úÖ ƒê√£ l·ªçc {len(filtered)} b√†i li√™n quan ƒë·∫øn kh√¥ng kh√≠ v√†o '{output_file}'")
            print(f"‚úÖ ƒê√£ chu·∫©n h√≥a c·ªôt th·ªùi gian cho t·∫•t c·∫£ c√°c b√†i vi·∫øt")
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b√†i n√†o li√™n quan.")
    except Exception as e:
        print(f"‚ùå L·ªói khi l·ªçc b√†i vi·∫øt: {e}")

# ==== H√†m l∆∞u d·ªØ li·ªáu v√†o database ====
def save_to_database(json_file):
    """L∆∞u d·ªØ li·ªáu t·ª´ file JSON v√†o TimescaleDB"""
    try:
        # ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not data:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u v√†o database")
            return False
        
        # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh DataFrame
        df = pd.DataFrame(data)
        
        # Chuy·ªÉn ƒë·ªïi c·ªôt th·ªùi gian
        df['time'] = pd.to_datetime(df['standardized_time'])
        
        # T·∫°o k·∫øt n·ªëi ƒë·∫øn TimescaleDB
        db_util = TimescaleDBUtil()
        if not db_util.connect():
            print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn TimescaleDB")
            return False
        
        # L·ªçc c√°c c·ªôt c·∫ßn thi·∫øt
        if 'standardized_time' in df.columns:
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u v√†o database
            news_df = df[['title', 'time', 'category', 'content', 'link', 'keyword']]
        else:
            print("‚ùå D·ªØ li·ªáu kh√¥ng c√≥ c·ªôt standardized_time")
            return False
        
        # L∆∞u v√†o database
        result = db_util.create_table_from_dataframe(
            df=news_df,
            table_name="news",
            time_column="time",
            schema="public",
            if_exists="append"
        )
        
        # ƒê√≥ng k·∫øt n·ªëi
        db_util.disconnect()
        
        if result:
            print(f"‚úÖ ƒê√£ l∆∞u {len(news_df)} b√†i vi·∫øt v√†o b·∫£ng news trong database")
            return True
        else:
            print("‚ùå L∆∞u d·ªØ li·ªáu v√†o database th·∫•t b·∫°i")
            return False
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u d·ªØ li·ªáu v√†o database: {e}")
        return False

# ==== H√†m ch√≠nh ====
def main():
    # ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu
    raw_data_file = "data/air_quality_news.json"
    filtered_data_file = "data/air_quality_filtered.json"
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs("data", exist_ok=True)

    # B∆∞·ªõc 1: Crawl d·ªØ li·ªáu
    print("üóÇ ƒêang ƒë·ªçc d·ªØ li·ªáu ƒë√£ l∆∞u...")
    existing_links = read_existing_links(raw_data_file)

    print("üöÄ B·∫Øt ƒë·∫ßu crawl c√°c b√°o...")
    all_data = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    for func in [crawl_vnexpress, crawl_vnexpress_ha_noi, crawl_dantri]:
        try:
            raw_data = func(existing_links=existing_links)
            for item in raw_data:
                item["crawled_date"] = today
                all_data.append(item)
                if "link" in item:
                    existing_links.add(item["link"])
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ch·∫°y h√†m {func.__name__}: {e}")

    # L∆∞u d·ªØ li·ªáu v√†o file JSON
    save_to_json(all_data, raw_data_file)
    print(f"‚úÖ ƒê√£ l∆∞u {len(all_data)} b√†i m·ªõi v√†o {raw_data_file}")
    
    # B∆∞·ªõc 2: L·ªçc b√†i vi·∫øt li√™n quan ƒë·∫øn kh√¥ng kh√≠
    print("üîç ƒêang l·ªçc b√†i vi·∫øt li√™n quan ƒë·∫øn kh√¥ng kh√≠...")
    filter_air_articles(raw_data_file, filtered_data_file)
    
    # B∆∞·ªõc 3: L∆∞u v√†o database
    print("üíæ ƒêang l∆∞u d·ªØ li·ªáu v√†o database...")
    save_to_database(filtered_data_file)

if __name__ == "__main__":
    main() 